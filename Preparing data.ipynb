{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preparing data\n",
    "\n",
    "Read document -> clean text -> convert to phrases -> labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "from docx import Document\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read documents and clean text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that read docx file and save as text. Remove special punctuations and characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function \n",
    "def read_file(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        sentence = para.text\n",
    "        sentence = sentence.lower()  # Converting to lowercase\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        sentence = re.sub(cleanr, ' ', sentence)  #Removing HTML tags\n",
    "        sentence = re.sub(r'[?|!|\\'|\"|#|,|)|(|\\|/$%\\n\\t.:;\"\"*]',r'',sentence) #Removing Punctuations\n",
    "        fullText.append(sentence)\n",
    "    while '' in fullText:\n",
    "        fullText.remove('') # remove empty line\n",
    "    return fullText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all document inside the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_folder/document_6.docx\n",
      "doc_folder/document_7.docx\n",
      "doc_folder/document_1.docx\n",
      "doc_folder/document_2.docx\n",
      "doc_folder/document_3.docx\n",
      "doc_folder/document_8.docx\n",
      "doc_folder/document_4.docx\n",
      "doc_folder/document_5.docx\n",
      "Total number of files 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = \"doc_folder/\"\n",
    "\n",
    "filesno=0\n",
    "text = []\n",
    "filelist = os.listdir(directory)\n",
    "\n",
    "for docname in filelist:\n",
    "    fullpath = directory+docname\n",
    "    if fullpath.endswith(\"docx\"):\n",
    "        print(fullpath)\n",
    "        filesno+=1\n",
    "        text += read_file(fullpath)\n",
    "        \n",
    "print(\"Total number of files\",filesno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for windows\n",
    "# import io \n",
    "# with io.open(\"\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert long text into short phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These function below are for supporting'text_to_phrases' function\n",
    "'''\n",
    "# identify all possible phrases\n",
    "def key_words_phrases(raw):\n",
    "    ngramlist=[]\n",
    "    x=minlen\n",
    "    ngramlimit = maxlen\n",
    "    tokens=nltk.word_tokenize(raw)\n",
    "\n",
    "    while x <= ngramlimit:\n",
    "        ngramlist.extend(nltk.ngrams(tokens, x))\n",
    "        x+=1\n",
    "    return ngramlist\n",
    "\n",
    "# join words into a new list\n",
    "def concat_words(wordlist):\n",
    "    new_list = []\n",
    "    for words in wordlist:\n",
    "        new_list.append(' '.join(words))   \n",
    "    return new_list\n",
    "\n",
    "'''\n",
    "text into phrases\n",
    "'''\n",
    "# define maximum and minimum number of words in one phrase\n",
    "maxlen = 10 \n",
    "minlen = 6 \n",
    "\n",
    "def text_to_phrases(text):\n",
    "    phrases = []\n",
    "    for sentence in text:\n",
    "        if len(str(sentence).split(' ')) <= maxlen:\n",
    "            phrases.append(sentence) \n",
    "        else:\n",
    "            wordlist = key_words_phrases(sentence)\n",
    "            phrases += concat_words(wordlist)\n",
    "    \n",
    "    print(len(phrases))\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189059\n"
     ]
    }
   ],
   "source": [
    "# list of phrases\n",
    "phrases = text_to_phrases(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------- Until here is read text in the documents and convert text into phrases only---------\n",
    "\n",
    "## ------------- Bellow is ---------------\n",
    "- import labeled text(long text) \n",
    "- clean text\n",
    "- convert long sentences into short phrases\n",
    "- labelling phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labeled text\n",
    "traina = pd.read_csv(\"labeled_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186760, 2)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10\n",
    "minlen = 6\n",
    "\n",
    "phrases = []\n",
    "labels = []\n",
    "\n",
    "for i in range(train.shape[0]):\n",
    "    sentence = train.loc[i,'text']\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[?|!|\\'|\"|#|,|)|(|\\|/$%\\n\\t.:;\"\"‘’]',r'',sentence)\n",
    "    \n",
    "    label = []\n",
    "    \n",
    "    if len(str(sentence).split(' ')) <= maxlen: # short sentence do not need to break down phrases\n",
    "        phrases.append(sentence) \n",
    "        \n",
    "        labels.append(train.loc[i,'label']) # append label to the phrase with text label\n",
    "        \n",
    "    else:\n",
    "        wordlist = key_words_phrases(sentence)  # break down long sentence to several phrases\n",
    "        phrases += concat_words(wordlist)\n",
    "        \n",
    "        label.append(train.loc[i,'label'])\n",
    "        labels += label*len(wordlist)   # append same labels to all the phrases from the same text.\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['phrases'] = phrases  \n",
    "train_df['labels'] = labels\n",
    "\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "train_df.to_csv(\"train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
