{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy NLP Model to Azure\n",
    "### Model is already trained on local environment and use SDK to deploy to Azure.\n",
    "\n",
    "### Key Take Away:\n",
    "-  For NLP model, Tokenizer and Neural Network Model, both need to be pickled and register models in workspace. It should be in 'init() module'.\n",
    "- Before text data go into the model, preprocessing could be done in 'run()' module. \n",
    "\n",
    "step 1,2,3 were done in local environment, step 4,5,6 were done in Azure.\n",
    "\n",
    "--- Local miniconda Jupyter Notebook\n",
    "- 1. Create Workspace\n",
    "- 2. Pickle the model\n",
    "- 3. Register the model\n",
    "\n",
    "--- Azure Jupyter Notebook\n",
    "- 4. Create Image\n",
    "- 5. Deploy WebService\n",
    "- 6. Test calling web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.conda_dependencies import CondaDependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If already created workspace, you can skip creating workspace part, and go to initialize workspace step.\n",
    "'''\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name='myworkspace',\n",
    "                      subscription_id='here is your Azure subcription id', \n",
    "                      resource_group='my_resource_group_name',\n",
    "                      create_resource_group=True,\n",
    "                      location='Australia East' \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write configuration json file\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"subscription_id\": \"d2f07f04-8cd1-4fcf-a2cd-da597ebb0cdc\",\r\n",
      "    \"resource_group\": \"NLP_RegTech\",\r\n",
      "    \"workspace_name\": \"myworkspace\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ./config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace\n",
    "If already created workspace, you can skip creating workspace part \n",
    "and just need to initialize workspace before do the rest of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /Users/hua/Desktop/NLP/aml_config/config.json\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the model and tokenizer\n",
    "For pickle details, please check: [Part 2 file : NLP Model_Text multi-classification.ipynb](https://github.com/Sweetflowerjulia/NLP-Model_Text-Category-with-GloVe-and-LSTM/blob/master/NLP%20Model_Text%20multi-classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "Register two models: prediction model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model model_600\n"
     ]
    }
   ],
   "source": [
    "model = Model.register(model_path = \"model.pkl\",\n",
    "                       model_name = \"model_600\",\n",
    "                       tags = {\"key\": \"1\"},\n",
    "                       description = \"Category Prediction\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model tokenizer\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Model.register(model_path = \"tokenizer.pickle\",\n",
    "                       model_name = \"tokenizer\",\n",
    "                       tags = {\"key\": \"2\"},\n",
    "                       description = \"tokenizer\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check all models in the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizer \tVersion: 1 \tDescription: tokenizer {'key': '2'}\n",
      "Name: model_600 \tVersion: 1 \tDescription: Objective Category Prediction {'key': '1'}\n"
     ]
    }
   ],
   "source": [
    "models = Model.list(workspace=ws)\n",
    "for m in models:\n",
    "    if m.name == \"model_600\":\n",
    "        model = m\n",
    "    if m.name == \"tokenizer\":\n",
    "        tokenizer = m\n",
    "    print(\"Name:\", m.name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Registered Model, you can see the models in your Azure workspace 'Model' tab.\n",
    "## ------- From here done in Azure Jupyter Notebook --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "# import library\n",
    "import pickle, json\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Conv1D, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# Other\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model,tokenizer\n",
    "    model_path = Model.get_model_path(model_name = \"model_600\")\n",
    "    tokenizer_path = Model.get_model_path(model_name = \"tokenizer\")\n",
    "    model,tokenizer = pickle.load(open(model_path,\"rb\")), pickle.load(open(tokenizer_path,\"rb\"))\n",
    "    \n",
    "\n",
    "def run(raw_data):\n",
    "    \n",
    "    try:\n",
    "        text = json.loads(raw_data)[\"data\"]\n",
    "        \n",
    "        prob = 0.5\n",
    "        minlen = 6\n",
    "        maxlen = 10\n",
    "        \n",
    "       '''\n",
    "       Preprocessing text before put into the model.\n",
    "       '''  \n",
    "        # clean text\n",
    "        text = text.lower()     # Converting to lowercase\n",
    "        text = re.sub(r'[?|!|\\'|\"|#|,|)|(|\\|/$%\\n\\t.:;\"\"‘’]',r'',text)\n",
    "        \n",
    "        # text to phrases\n",
    "        word_list = text.split(' ')\n",
    "        len_text = len(word_list)\n",
    "        \n",
    "        phrases = []   \n",
    "        len_each_phrase = list(set([minlen,(maxlen + minlen)//2, maxlen]))    \n",
    "\n",
    "        i = 0\n",
    "        while i <= len_text:\n",
    "            for nword in len_each_phrase:    \n",
    "                if i+nword >= len_text:\n",
    "                    phrase = ' '.join(word_list[i:])\n",
    "                    phrases.append(phrase)\n",
    "                    i = len_text\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    phrase = ' '.join(word_list[i:i+nword])\n",
    "                    phrases.append(phrase)\n",
    "            i += 1\n",
    "            \n",
    "        # tokenize phrases (text -> numerical value)\n",
    "        sequences = tokenizer.texts_to_sequences(phrases) #phrases\n",
    "        test_data = pad_sequences(sequences, maxlen = maxlen)\n",
    "        \n",
    "        # predict with model\n",
    "        preds = model.predict(test_data)\n",
    "\n",
    "        preds_df = pd.DataFrame(preds)\n",
    "        count = preds_df[preds_df > prob].count()\n",
    "        \n",
    "        # return label based on probability\n",
    "        if sum(count[1:] > 0) > 0:\n",
    "            result = count.idxmax()\n",
    "        else:\n",
    "            result = 0\n",
    "\n",
    "        return json.dumps({\"category\": result}) \n",
    "    \n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"category\": result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=[\"numpy\",\"keras\",\"pandas\"]) #\"nltk.download('punkt')\",\n",
    "\n",
    "# myenv = CondaDependencies()\n",
    "myenv.add_pip_package(\"pynacl==1.2.1\")\n",
    "\n",
    "with open(\"fullenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.image import Image, ContainerImage\n",
    "\n",
    "image_config = ContainerImage.image_configuration(runtime= \"python\",\n",
    "                                 execution_script=\"score.py\",\n",
    "                                 conda_file=\"fullenv.yml\")\n",
    "\n",
    "image = Image.create(name = \"myimage2\",\n",
    "                     # this is the model object \n",
    "                     models = [model,tokenizer],\n",
    "                     image_config = image_config, \n",
    "                     workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running.....................................................\n",
      "SucceededImage creation operation finished for image myimage2:8, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "image.wait_for_creation(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy WebService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 2, \n",
    "                                               memory_gb = 2, \n",
    "                                              auth_enabled = True) # auth_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-aci-service-3\n",
      "Creating service\n",
      "Running............................\n",
      "SucceededACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "service_name = 'my-aci-service-3'\n",
    "print(service_name)\n",
    "service = Webservice.deploy_from_image(deployment_config = aciconfig,\n",
    "                                           image = image,\n",
    "                                           name = service_name,\n",
    "                                           workspace = ws)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test WebService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"category\": 7}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "test_sample = json.dumps({'data': 'receive ............. your life'})\n",
    "\n",
    "prediction = service.run(input_data=test_sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the important imformation that call the webservice.\n",
    "- Scoring_uri\n",
    "- Auth_Key (primary key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary, secondary = service.get_keys()\n",
    "print(primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Calling the service using AuthKey and URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{\\\"category\\\": 7}\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# URL for the web service\n",
    "scoring_uri = 'XXX....XX'\n",
    "# If the service is authenticated, set the key\n",
    "key = 'XXX......XX'\n",
    "\n",
    "# Two sets of data to score, so we get two results back\n",
    "data = {\"data\": \"receive ............. your life\"    \n",
    "        }\n",
    "# Convert to JSON string\n",
    "input_data = json.dumps(data)\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "# If authentication is enabled, set the authorization header\n",
    "headers['Authorization']=f'Bearer {key}'\n",
    "\n",
    "# Make the request and display the response\n",
    "result = requests.post(scoring_uri, input_data, headers = headers)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
